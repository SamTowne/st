## Machine Learning Engineering

### Objective
Understand machine learning to help with these types of things.
  - communicate and diagram the architecture and foundational components needed to implement a use case
  - influence ML engineering cost control, staying on budget and reducing financial risk
  - provide training cost estimates, given a dataset and ML infrastructure
  - build and continuously deploy a ML application in AWS

### Apache Airflow
- find some public, mature apache airflow usage with CICD that enables the promotion of DAGs from lower to higher envs https://github.com/jghoman/awesome-apache-airflow
- try and find out if people are shceduling to shut down environments after hours, and if any of those using the aws automation for off hours shutdown are also using terraform for iac

#### Hands on projects to consider
- https://towardsdatascience.com/a-weekend-ai-project-running-speech-recognition-and-a-llama-2-gpt-on-a-raspberry-pi-5298d6edf812

#### Stas00 online book
Practical strategy for large scale ML engineering.
- stopped at IO

#### Machine Learning Engineering in Action - Ben Wilson
- 2.4 The Foundation of ML engineering
- the MLOps paradigm and the similarities between it, DevOps, and agile development
- the data science flavor of devops is MLOps
- MLOps engages a lot of stuff to help avoid "failed, cancelled and non-adopted solutions"
  - Monitoring -> prediction accuracy, retraining performance, impact assessment, fallback/failure rate, logging
  - Release -> versioning + registration, change managenment, approvals, auditing, A/B testing + statistical evaluation, bandit algorithms
    - there is this concept of Data Version Control, DVC, which is basically git for data sets, https://towardsdatascience.com/introduction-to-data-version-control-59fabf447a60
  - CI/CD -> build tools for continuous integration, deployment tools for continuous deployment
  - Code development practices -> source code management, branching strategies, feature merging, parameter and metrics tracking, project experimentation and tuning tracking
  - Code review practices -> peer review, unit test coverage, metrics validation
  - Environment configuration -> Dev, QA, Prod, infrastructure management, platform management, dependency library management
  - Artifact management -> Artifact repositories, registration, QA simulation tests, UAT
  - Software engineering standards and Agile methods
- "ML engineering brings the core functional capabilities of a data scientist, a data engineer, and a software engineer into a hybrid role that supports the creation of ML solutions focused on solving a problem through the rigors of professional software development."
- "Delivering the simplest possible solution helps reduce development, computational, and operational costs for any given project."
- "Borrowing and adapting Agile fundamentals to ML project work helps shorten the development life cycle, forces development architectures that are easier to modify, and enforces testability of complex applications to reduce maintenance burdens."
- "Just as DevOps augments software engineering work, MLOps augments ML engineering work. While many of the core concepts remain the same for these paradigms, additional aspects of managing model artifacts and performing continuous testing of new versions introduce nuanced complexities."
- 3.0 Before you model: Planning and scoping a project
- find the simplest solution, early
- aren't project planning and agile at odds?
  - "A project plan is important, but it must not be too rigid to accomodate changes in technology or the environment, stakeholders' priorities, and people's understanding of the problem and its solution." - Scott Ambler, agile manifesto essay
  - "Planning is good in ML. It's just critical to not set those plans in stone."
- Tells a story of a data science team who has had multiple successesful projects at their company
  - However, all of their previous projects were able to be completed in isolation, for the most part, from the rest of the business
  - For the latest project, executives want them to add purchase recommendations to the company website
  - The team begins their work, as usual, studiously working in isolation and making assumptions that the executives are envisioning something siilar to the website recommendations which competition has implemented
  - They test dozens of implementations, consume hundreds of papers, and build out an MVP "using alternating least squares ALS that achieces a root mean squared error RMSE of 0.2334 along with a rough implementation of ordered scoring for relevance based on prior behavior."
  - The team is stoked as they head to meet with the business sponsors about the MVP
  - The response from the business is not great
  - They ask lots of questions and do not seem to understand the functionality of the solution
  - Attempts to make the solution more clear to the sponsors cause even more confusion and ambiguity about the solution
  - The team did not properly plan for the nuances of the project, and the MVP was received negatively due to this gap
  - They probably needed additional input from company sponsors, such as marketing, to frame the problem better during planning
  - The recommendations the solution made were not quite nuanced enough to impact customer decisions in a logical way
  - The DS team needed external input about the product being sold and typical customer behavior before getting too deep into the technical implementation
  - Don't blindly trust your metrics
  - Supplement the model-scoring metrics with other, subjective, means of understanding model efficacy
  - Sometimes just drawing a diagram of the solution, and the user experience of the solution, can bring improvement options into view
  - The data science team from the story would have done much better had they asked some of the marketing team to provide visualizations of what the new customer experience might look like with the new feature
  - A business analyst reviewed the proposed solution and found multiple problems "the nuances of the problem"
    - there was duplicated item data due to older product IDs
    - different divisions used different product IDs and this was not being accounted for
- 3.1.1 Basic planning for a project
- that first meeting
- there is a tendency to begin thinking about the "how" of a project in the first meeting
- avoid doing this, instead, listen carefully and take notes, really try to understand the current need with a 'teach me how you do it now please' approach
- 3.1.3 Plat for demos - lots of demos
- try and demo each feature as they are available if possible
  - for an MVP with 4 features, here would be a good demo strategy -> model testing , demo, feature 1 , demo, feature 2, demo, feature 3 demo, feature 4, full MVP demo
  - you don't want to build the entire MVP and then demo to stakeholders, what if something was missed? but this too is based upon MVP complexity, if it's simple it may make sense to whip it out and then demo
  - "without frequent demos as features are built out, the team at large is simply operating in the dark with respect to the ML aspect of the project. The ML team, meanwhile, is missing out on valuable time-saving feedback from SME members..."
  - keep individual feature demos lightweight! a slide deck to get the point across for each feature is BETTER than building a microservice or frontend that takes quadruple the time/effort and slows the project timeline... save that time for the bigger demos as multiple features come together
- 3.1.4 Experimentation by solution building: Wasting time for pride's sake
- avoid solution-building during experimentation such as software bake-offs where the team splits up into groups to explore each solution, this hackathon-like approach is not good for ml engineering
- concept of multiple-MVP development vs. experimentation culling development
- instead, leverage prototype experimentation enables the team to find an MVP to continue pursuing together, instead of wasting effort building 3 MVPs, then continuing with the winner
- experimentation culling is to consider the MVP options, and do low-cost prototypes as a team to decide which one to turn into the actual MVP for the project
- experimentation is important for ML projects, but "must be time-boxed"
- 3.2 Experimental scoping: Setting expectations and boundaries
- "Through setting these expectations and providing boundaries on each of them (for both time and level of implementation complexity), the ML team can provide the one thing that the business is seeking: an expected delivery date and a judgement call on what is or isn't feasible."
- 3.2.1 What is experimental scoping?
- After requirements gathering, it is absolutely essential to set an expectation with the larger team on how long the DS team will spend on vetting each of the ideas during research and experimentation.
- Having a target date helps bring focus.
- Having a date for something that is wholly unknowable (the best solution) may seem counterintuitive, but it forces simplicifty and focused thought around finding options within a time box
- The time box on the initial experimentation naturally forces teams to postpone evaluation of things that are "interesting" but not essential, jot them down somewhere, parking lot them, and reevaluate them down the road
- For complex stuff, it may require 2 weeks just for research followed by another 2 weeks for hacking
- the sole purpose of these exercises is to decide on a path, and to make that decision in the shortest time possible against the timeline of delivery for the MVP
- the amount of time required depends on the complexity of the business need, and as a team gets more comfortable with the business and each other, it becomes easier to provide a timeline to the business for the "experiment/hacking" phase
- 3.2.2 Experimental scoping for the ML team: Research
- the depth and breadth of the ML space is a black hole
- time box the research efforts to avoid over-spending project time in this phase
- set boundaries about how long and how far the research will go for a new problem
- a research path for a team often looks like this
  - individual research (ask peers, review technical books, etc.)
  - data science team evaluation of options provided by each individual (ideally both classic and proven implementations and state of the art methods are in consideration)
  - selection of top 3 options to run experimentation on (adjudication and culling process)
- the outcome of the research path should be a few options to run prototype development against
- prototype development is also time boxed, these are intended to hash out the options for an MVP decision
- then, from the prototype development, a MVP is formed
- 3.2.3 Experimental scoping for the ML team: Experimentation
- project lead should set the expectation for experimentation
- "The goal is to produce a simulation of the end product that allows for an unbiased comparison of the solutions being considered."
- "There is no need for the models to be tuned, nor for the code to be written in such a way that it could ever be considered for use in the final project's code base."
- "The name of the game here is a balance between two primary goals: speed and comparability."
- evaluate the performance of the solutions AND the difficulty of developing the full solution
- estimate the total final code complexity so that the larger team can be informed about estimated development time for each option
- this estimation process is important for helping the business to understand total cost of ownership of the solution options
- this is a good opportunity to pause, and demo the options
- the results of a solution are not the only consideration, there is also complexity risk that should be considered, is a 15% better result worth 80% more complexity and dev time?
- "If an appreciable gain isn't clearly obvious to everyone (including the business), an internal discussion should take place about resume-driven development and the motice for taking on such increased work."
- "Everyone just needs to be aware of what they're getting into and what they'll potentially be maintianing for a few years should they choose to pursue this additional complexity."
- experimentation plan diagrams are useful to share the expectation about how to experiment in a time-boxed way, example "manually create datasets vs. writing some complex ETL job to automate it during experimentation"
- it should be clear that the purpose of experimentation is to obtain a working option, NOT to find the optimal solution, simplify scope, lower barrier to entry, tried and true options reduce risk
- the optimal solution might actually look like months or years of research, publishing papers, optimizing a library or writing a new one lol, clarify the scope of experimentation to match the actual business need. 
- "are we google and need global scale and machine-code level optimization? or are we a regular fortune 500 with normal needs?"
- "the goal of experimentation is to find the most promising and simplest approach that solves the problem, not to use the most technically sophisticated solution."
- questinos that help uncover total cost of ownership of an option
  - what additional ETL do we need to build?
  - how often do we need to retrain models and generate interfaces?
  - what platform are we going to sue to run these models?
  - for the platform and infrastructure that we need, are we going to use a managed service or are we going to try and run it ourselves?
  - do we have expertise in running and maintaining services for ML of this nature?
  - what is the cost of the serving layer plans that we have?
  - what is the cost of storage, and where will the inference data live to support this project?
- these questions do not need to be answered before development starts, but they should be continually evaluated throughout
- sometimes it'd be better to decide earlier and fail then to evaluate options for too long and fail due to time investment, oh look its the amazing fail fast buzzword
- using a weighted decision matrix for options analysis
- 4 Before you model: Communication and logistics of projects
- key questions
  - why do you want this built?
  - what do you expect a solution to do?
  - how does your team do this now?
  - what would you consider to be a perfect solution for this?
  - how much would you pay for another company to do this for you?
  - when would this solution become irrelevant?
- 4.1 Communication: Defining the problem
- don't forget to ask the why question
- why do the project?
- collectively discuss the project's key principles to identify the
- "simplest possible solution that solves the problem"
- 4.1.1
